{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3586,"sourceType":"datasetVersion","datasetId":2134},{"sourceId":7378735,"sourceType":"datasetVersion","datasetId":4287904}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-10T12:59:14.422989Z","iopub.execute_input":"2024-03-10T12:59:14.423717Z","iopub.status.idle":"2024-03-10T12:59:15.863847Z","shell.execute_reply.started":"2024-03-10T12:59:14.423671Z","shell.execute_reply":"2024-03-10T12:59:15.862756Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/wordnet/wordnet/data.adj\n/kaggle/input/wordnet/wordnet/LICENSE\n/kaggle/input/wordnet/wordnet/index.adj\n/kaggle/input/wordnet/wordnet/README\n/kaggle/input/wordnet/wordnet/verb.exc\n/kaggle/input/wordnet/wordnet/index.adv\n/kaggle/input/wordnet/wordnet/index.verb\n/kaggle/input/wordnet/wordnet/data.verb\n/kaggle/input/wordnet/wordnet/lexnames\n/kaggle/input/wordnet/wordnet/data.noun\n/kaggle/input/wordnet/wordnet/noun.exc\n/kaggle/input/wordnet/wordnet/index.noun\n/kaggle/input/wordnet/wordnet/adj.exc\n/kaggle/input/wordnet/wordnet/index.sense\n/kaggle/input/wordnet/wordnet/adv.exc\n/kaggle/input/wordnet/wordnet/cntlist.rev\n/kaggle/input/wordnet/wordnet/data.adv\n/kaggle/input/wordnet/wordnet/citation.bib\n/kaggle/input/wordnet/wordnet/wordnet/data.adj\n/kaggle/input/wordnet/wordnet/wordnet/LICENSE\n/kaggle/input/wordnet/wordnet/wordnet/index.adj\n/kaggle/input/wordnet/wordnet/wordnet/README\n/kaggle/input/wordnet/wordnet/wordnet/verb.exc\n/kaggle/input/wordnet/wordnet/wordnet/index.adv\n/kaggle/input/wordnet/wordnet/wordnet/index.verb\n/kaggle/input/wordnet/wordnet/wordnet/data.verb\n/kaggle/input/wordnet/wordnet/wordnet/lexnames\n/kaggle/input/wordnet/wordnet/wordnet/data.noun\n/kaggle/input/wordnet/wordnet/wordnet/noun.exc\n/kaggle/input/wordnet/wordnet/wordnet/index.noun\n/kaggle/input/wordnet/wordnet/wordnet/adj.exc\n/kaggle/input/wordnet/wordnet/wordnet/index.sense\n/kaggle/input/wordnet/wordnet/wordnet/adv.exc\n/kaggle/input/wordnet/wordnet/wordnet/cntlist.rev\n/kaggle/input/wordnet/wordnet/wordnet/data.adv\n/kaggle/input/wordnet/wordnet/wordnet/citation.bib\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-semcorraw-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-shaks-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-treebank-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-bnc-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/wn30compounds.txt\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-brown-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-shaks-resnink-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-brown-resnik-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-semcorraw-resnik-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/README\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-bnc-resnik-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/IC-semcor.sh\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-semcorraw.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-semcor.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-semcor-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-bnc-resnik.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-treebank.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-shaks.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-bnc.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-semcorraw-resnik.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-treebank-resnik-add1.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-brown-resnik.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/IC-compute.sh\n/kaggle/input/wordnet/wordnet/wordnet_ic/shaks12.txt\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-treebank-resnik.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/stoplist.txt\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-shaks-resnik.dat\n/kaggle/input/wordnet/wordnet/wordnet_ic/ic-brown.dat\n/kaggle/input/wordnet/wordnet_ic/ic-semcorraw-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-shaks-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-treebank-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-bnc-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wn30compounds.txt\n/kaggle/input/wordnet/wordnet_ic/ic-brown-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-shaks-resnink-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-brown-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-semcorraw-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/README\n/kaggle/input/wordnet/wordnet_ic/ic-bnc-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/IC-semcor.sh\n/kaggle/input/wordnet/wordnet_ic/ic-semcorraw.dat\n/kaggle/input/wordnet/wordnet_ic/ic-semcor.dat\n/kaggle/input/wordnet/wordnet_ic/ic-semcor-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-bnc-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/ic-treebank.dat\n/kaggle/input/wordnet/wordnet_ic/ic-shaks.dat\n/kaggle/input/wordnet/wordnet_ic/ic-bnc.dat\n/kaggle/input/wordnet/wordnet_ic/ic-semcorraw-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/ic-treebank-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/ic-brown-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/IC-compute.sh\n/kaggle/input/wordnet/wordnet_ic/shaks12.txt\n/kaggle/input/wordnet/wordnet_ic/ic-treebank-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/stoplist.txt\n/kaggle/input/wordnet/wordnet_ic/ic-shaks-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/ic-brown.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-semcorraw-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-shaks-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-treebank-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-bnc-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/wn30compounds.txt\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-brown-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-shaks-resnink-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-brown-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-semcorraw-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/README\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-bnc-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/IC-semcor.sh\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-semcorraw.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-semcor.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-semcor-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-bnc-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-treebank.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-shaks.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-bnc.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-semcorraw-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-treebank-resnik-add1.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-brown-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/IC-compute.sh\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/shaks12.txt\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-treebank-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/stoplist.txt\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-shaks-resnik.dat\n/kaggle/input/wordnet/wordnet_ic/wordnet_ic/ic-brown.dat\n/kaggle/input/human-vs-llm-text-corpus/distribution.parquet\n/kaggle/input/human-vs-llm-text-corpus/data.parquet\n/kaggle/input/human-vs-llm-text-corpus/distribution.csv\n/kaggle/input/human-vs-llm-text-corpus/prompts.csv\n/kaggle/input/human-vs-llm-text-corpus/data.csv\n/kaggle/input/human-vs-llm-text-corpus/prompts.parquet\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\nimport pandas as pd\nimport nltk\nfrom nltk import WordNetLemmatizer, WordPunctTokenizer\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:56:13.691315Z","iopub.execute_input":"2024-03-10T13:56:13.691907Z","iopub.status.idle":"2024-03-10T13:56:13.698976Z","shell.execute_reply.started":"2024-03-10T13:56:13.691864Z","shell.execute_reply":"2024-03-10T13:56:13.697832Z"},"trusted":true},"execution_count":218,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2024-03-10T12:59:21.874874Z","iopub.execute_input":"2024-03-10T12:59:21.875648Z","iopub.status.idle":"2024-03-10T12:59:23.509695Z","shell.execute_reply.started":"2024-03-10T12:59:21.875599Z","shell.execute_reply":"2024-03-10T12:59:23.507584Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/human-vs-llm-text-corpus/data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T12:59:23.511622Z","iopub.execute_input":"2024-03-10T12:59:23.512899Z","iopub.status.idle":"2024-03-10T13:00:22.894055Z","shell.execute_reply.started":"2024-03-10T12:59:23.512859Z","shell.execute_reply":"2024-03-10T13:00:22.892766Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:00:22.897635Z","iopub.execute_input":"2024-03-10T13:00:22.898048Z","iopub.status.idle":"2024-03-10T13:00:23.041087Z","shell.execute_reply.started":"2024-03-10T13:00:22.898017Z","shell.execute_reply":"2024-03-10T13:00:23.039471Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           prompt_id    text_length     word_count\ncount  788922.000000  788922.000000  788922.000000\nmean      420.069514    3123.374844     497.983404\nstd      1381.285340    4963.074433     720.866585\nmin         0.000000     105.000000      25.000000\n25%         0.000000     647.000000     110.000000\n50%         0.000000    1921.000000     324.000000\n75%         0.000000    3910.000000     624.000000\nmax      9913.000000  890119.000000   71543.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_id</th>\n      <th>text_length</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>788922.000000</td>\n      <td>788922.000000</td>\n      <td>788922.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>420.069514</td>\n      <td>3123.374844</td>\n      <td>497.983404</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1381.285340</td>\n      <td>4963.074433</td>\n      <td>720.866585</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>105.000000</td>\n      <td>25.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>647.000000</td>\n      <td>110.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>1921.000000</td>\n      <td>324.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>3910.000000</td>\n      <td>624.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9913.000000</td>\n      <td>890119.000000</td>\n      <td>71543.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Примерный план решения задачи генерации следующего слова на основе нескольких предыдущих слов\n# Количество предыдущих слов - 5\n# На основе этой модели будем генерировать предложения в цикле while true \n# с условием на длину предложения не больше max_length\n# Модель будем просто представлять собой embedding -> lstm layer -> linear layer -> softmax\n# Обучим наши собственные embeddings и измерим на них nll_avg_loss\n# Обучим модель на предобученных embeddings и измерим nll_avg_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:00:23.042923Z","iopub.execute_input":"2024-03-10T13:00:23.043424Z","iopub.status.idle":"2024-03-10T13:00:23.049220Z","shell.execute_reply.started":"2024-03-10T13:00:23.043384Z","shell.execute_reply":"2024-03-10T13:00:23.048140Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"texts = data.loc[:].text.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:00:23.050728Z","iopub.execute_input":"2024-03-10T13:00:23.051772Z","iopub.status.idle":"2024-03-10T13:00:23.111357Z","shell.execute_reply.started":"2024-03-10T13:00:23.051713Z","shell.execute_reply":"2024-03-10T13:00:23.110317Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def tokenize_lemmatize(texts, tokenizer, lemmatizer):\n    tokenized = [tokenizer.tokenize(sentence) for sentence in texts]\n    lemmatized = [[lemmatizer.lemmatize(word) for word in sent] for sent in tokenized]\n    return lemmatized\ntokenizer = WordPunctTokenizer()\nlemmatizer = WordNetLemmatizer()\npreprocessed = tokenize_lemmatize(texts[:100], tokenizer, lemmatizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:00:23.112896Z","iopub.execute_input":"2024-03-10T13:00:23.113650Z","iopub.status.idle":"2024-03-10T13:00:26.296144Z","shell.execute_reply.started":"2024-03-10T13:00:23.113606Z","shell.execute_reply":"2024-03-10T13:00:26.294632Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"vocab = set([word for sent in preprocessed for word in sent])\nvocab.add('<end>')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:00:26.300815Z","iopub.execute_input":"2024-03-10T13:00:26.301747Z","iopub.status.idle":"2024-03-10T13:00:26.312814Z","shell.execute_reply.started":"2024-03-10T13:00:26.301702Z","shell.execute_reply":"2024-03-10T13:00:26.311010Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"itos = {i : word for i, word in enumerate(vocab)}\nstoi = {word : i for i, word in itos.items()}","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:00:26.314625Z","iopub.execute_input":"2024-03-10T13:00:26.314977Z","iopub.status.idle":"2024-03-10T13:00:26.329893Z","shell.execute_reply.started":"2024-03-10T13:00:26.314951Z","shell.execute_reply":"2024-03-10T13:00:26.328563Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# creating X and y tensors\ndef build_dataset(preprocessed):\n    X, y = [], []\n    window_size = 5\n    for sent in preprocessed:\n        sent.append('<end>')\n        left = 0\n        for right in range(window_size, len(sent)):\n            context = [stoi[word] for word in sent[left : right]]\n            positive_target = stoi[sent[right]]\n            X.append(context)\n            y.append([positive_target, 1])\n            left += 1\n    X, y = torch.tensor(X), torch.tensor(y)\n    # negative sampling\n    random_indices = torch.randperm(X.shape[0])[:len(preprocessed) * 20]\n    negative_samples = X[random_indices]\n    negative_targets = torch.zeros(size = (negative_samples.shape[0], 2))\n    negative_targets[:, 0] = torch.randint(0, len(vocab) - 1, size = (negative_samples.shape[0],))\n    \n    return torch.concat((X, negative_samples), dim = 0), torch.concat((y, negative_targets), dim = 0).int()\nX, y = build_dataset(preprocessed)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:02:13.819915Z","iopub.execute_input":"2024-03-10T14:02:13.820435Z","iopub.status.idle":"2024-03-10T14:02:14.072790Z","shell.execute_reply.started":"2024-03-10T14:02:13.820393Z","shell.execute_reply":"2024-03-10T14:02:14.071447Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"def nll(inputs, targets):\n    # inputs is the result after linear layer\n    # inputs: batch_size x C, C - number of targets\n    # targets: value where 0<=value<C, index of target\n    counts = inputs.exp()\n    preds = counts / counts.sum(1, keepdims = True)\n    return -(preds[torch.arange(preds.shape[0]), targets]).log().mean().item()\ndef nll_logprobs(log_probs, targets):\n    return -log_probs[torch.arange(log_probs.shape[0]), targets].mean().item()\n\ndef cross_entropy_loss(preds, targets):\n    # targets will be in format [target_idx, target_class]: target_class {0, 1}\n    l1, l2 = targets[targets[:, 1] == 1].shape[0], targets[targets[:, 1] == 0].shape[0]\n    # Calculate losses for ones and zeros\n    # Separate targets based on target_class\n    ones_targets = targets[targets[:, 1] == 1][:, 0]\n    zeros_targets = targets[targets[:, 1] == 0][:, 0]\n    ones_loss = zeros_loss = 0\n    e = 0.00001\n    if len(ones_targets) > 0:\n        ones_loss = -(preds[torch.arange(len(ones_targets)), ones_targets] + e).log().mean()\n\n    if len(zeros_targets) > 0:\n        zeros_loss = (preds[torch.arange(len(zeros_targets)), zeros_targets] + e).log().mean()\n    return (ones_loss + zeros_loss) / 2","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:02:15.168522Z","iopub.execute_input":"2024-03-10T14:02:15.169006Z","iopub.status.idle":"2024-03-10T14:02:15.182267Z","shell.execute_reply.started":"2024-03-10T14:02:15.168975Z","shell.execute_reply":"2024-03-10T14:02:15.180620Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"# create embedding function\ndef embedding(x, C):\n    return C[x]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:02:15.735935Z","iopub.execute_input":"2024-03-10T14:02:15.736398Z","iopub.status.idle":"2024-03-10T14:02:15.741707Z","shell.execute_reply.started":"2024-03-10T14:02:15.736365Z","shell.execute_reply":"2024-03-10T14:02:15.740817Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"C = torch.randn(len(vocab), 50) \ninput_dim = 50\nhidden_dim = 100\nseq_length = 5\nbatch_size = 64\nH = torch.randn(seq_length, batch_size, hidden_dim); h_0 = torch.randn(batch_size, hidden_dim)\nc_0 = torch.randn(1, batch_size, hidden_dim)\nW_x = torch.randn(4 * hidden_dim, input_dim).T\nW_h = torch.randn(4 * hidden_dim, hidden_dim).T\nb = torch.randn(4 * hidden_dim)\nW, b2 = torch.randn(100, len(vocab)), torch.randn(len(vocab))","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:05:58.925580Z","iopub.execute_input":"2024-03-10T14:05:58.926138Z","iopub.status.idle":"2024-03-10T14:05:58.945107Z","shell.execute_reply.started":"2024-03-10T14:05:58.926098Z","shell.execute_reply":"2024-03-10T14:05:58.944178Z"},"trusted":true},"execution_count":238,"outputs":[]},{"cell_type":"code","source":"# main part of model, LSTM layer\ndef lstm_cell(x, h, c, W_h, W_x, b):\n    # we are also going to handle batches in this implementation of lstm\n    i, f, g, o = torch.split(x @ W_x + h @ W_h + b, split_size_or_sections = h.shape[1] , dim = 1)\n    #print((x @ W_x + h @ W_h + b).shape)\n    i, f, g, o = F.sigmoid(i), F.sigmoid(f), torch.tanh(g), F.sigmoid(o)\n    c = f * c + i * g\n    h = o * torch.tanh(c).squeeze(0)\n    return h, c\ndef lstm(X, h = h_0, c = c_0, W_h = W_h, W_x = W_x, b = b):\n    for i in range(X.shape[0]):\n        h, c = lstm_cell(X[i], h, c, W_h, W_x, b)\n        H[i] = h\n    return H, c\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:05:59.049124Z","iopub.execute_input":"2024-03-10T14:05:59.049554Z","iopub.status.idle":"2024-03-10T14:05:59.058141Z","shell.execute_reply.started":"2024-03-10T14:05:59.049520Z","shell.execute_reply":"2024-03-10T14:05:59.056966Z"},"trusted":true},"execution_count":239,"outputs":[]},{"cell_type":"code","source":"parameters = [C, W, b2, b, W_x, W_h]\nfor p in parameters:\n    p.requires_grad = True\nfor p in parameters:\n    p.grad = None","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:05:59.479178Z","iopub.execute_input":"2024-03-10T14:05:59.479617Z","iopub.status.idle":"2024-03-10T14:05:59.486223Z","shell.execute_reply.started":"2024-03-10T14:05:59.479587Z","shell.execute_reply":"2024-03-10T14:05:59.485092Z"},"trusted":true},"execution_count":240,"outputs":[]},{"cell_type":"code","source":"losses = []\nfor i in range(100):\n    indices = torch.randint(0, X.shape[0], (64, ))\n    batch = embedding(X[indices], C).permute(1, 0, 2)\n    hidden = lstm(batch)[0][-1]\n    logits = hidden @ W + b2\n    logits = logits - logits.max(dim = 1)[0].view(-1, 1) # optimizing to reduce infinities\n    counts = logits.exp()\n    pred = counts / counts.sum(1, keepdims = True)\n    loss = cross_entropy_loss(pred, y[indices])\n    losses.append(loss.item())\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    for p in parameters:\n        p.data -= 0.1 * p.grad\n# RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). \n# Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). \n# Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n# add Codeadd Markdown","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:07:47.660482Z","iopub.execute_input":"2024-03-10T14:07:47.660903Z","iopub.status.idle":"2024-03-10T14:07:47.768113Z","shell.execute_reply.started":"2024-03-10T14:07:47.660876Z","shell.execute_reply":"2024-03-10T14:07:47.766331Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":244,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[244], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     13\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     16\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."],"ename":"RuntimeError","evalue":"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.","output_type":"error"}]},{"cell_type":"code","source":"r = range(len(losses))\nplt.plot(r, losses)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:55:59.107907Z","iopub.execute_input":"2024-03-10T13:55:59.108332Z","iopub.status.idle":"2024-03-10T13:55:59.113680Z","shell.execute_reply.started":"2024-03-10T13:55:59.108302Z","shell.execute_reply":"2024-03-10T13:55:59.112339Z"},"trusted":true},"execution_count":216,"outputs":[]}]}